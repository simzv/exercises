Реализованное решение заключается в построчном чтении исходного файла и
построчной записи в выходной файл.

Пример использования:
python fmt.py source_file.txt output_file.txt 

Запуск тестов (из папки c файлами решения):
python -m unittest discover

Каждая строка исходного файла просматривается посимвольно, в ней выделяются
слова, которые складываются во временный буфер, суммарная длина (в символах)
всех слов из буфера анализируется перед вставкой в буфер нового слова.

Если добавление нового слова в буфер приведет к тому, что длина слов в нем
станет больше, чем длина выходной строки (за минусом одинарных пробелов и
возможно отступа первой строки абзаца), то из этого буфера строится новая
строка и помещается в список готовых строк, а новое слово добавляется уже в
очищенный буфер слов.

Готовая строка может строиться из слов двумя способами: простым и с
юстифицированием. Простой способ очевиден - слова склеиваются через 
односимвольный пробел.

При юстификации вычисляется сколько нужно вставить пробелов. В зависимости от
количества слов в буфере определяется скольки символьные пробелы необходимо
встваить между словами. Наиболее вероятно, что общее число пробелов не делится
поровну на число промежутков между словами. Поэтому так же вычисляется то,
начиная с какого промежутка между слов необходимо вставлять дополнительно еще
один пробел. Зная все эти цифры последовательно формируется список слов и
пробелов (плюс отступ первой строки парагарфа если нужен), который затем
склеивается в одну строку.

Следует отметить, что в решении учитывается возможность работы с многобайтовыми
символами utf-8. Реализовано оптимистичное детектирование utf-8 кодировки и
длины символов. Задача распознавания utf-8 символов была одной из причин для
посимвольного разбора входной строки. При каждом новом символе, которого больше
127, происходит анализ его содержимого и содержимого последующих байтов на
соответствие схемы кодирования utf-8. Оптимистичность заключается в том, что
анализ последующих байтов многобайтового utf-8 символа осуществляется только
один раз. Если из этих байтов можно сложить utf-8 символ, то кодировка всего
текста принимается за utf-8. Если же сложить utf-8 символ не получается, то
кодировка текста принимается за не-utf8, то есть все символы далее трактуются
как однобайтовые и попыток детектирования utf-8 больше не предпринимается.
Наоборот, если же определилась кодировка utf-8, то все остальные символы
продолжают анализироваться, но только их первый байт, из которого выявляется
длина многобайтового символа, а второй и далее байты того же символа уже не
тестируются.

Очевидно, что длины входных и выходных строк чаще всего различны. Из одной
входной строки может получиться много выходных и наоборот. Поэтому выходные
строки выдаются списком, а не построчно.

И более того после парсинга последней (как и любой другой) строки во внутреннем
буфере могут остаться слова, из которых еще не была составлена строка (увы о
том, что строка действительно закончилась чаще всего можно узнать только по
содержимому следующей строки). А если следующей строки нет, то следует
форсировать построение неполной строки из буфера слов. Для этого реализован
необязательный параметр в методе получения списка готовых строк.

Решение не является универсальным и может приводить не к самому лучшему
форматированию в силу простоты правил форматирования.
В конце каждой строки ставится unix-like символ перевода строки, такой файл
будет затруднительно просмотреть в блокноте ос Windows.
Детектирование utf-8 символа может ошибиться, если в тексте есть экзотические
символы из какой-либо однобайтовой кодировки.
Так же не обрабатывается случай наличия символа bom в первой строке исходного
файла.
